"""
ì¬í•´ íŠ¸ìœ— ë¶„ë¥˜ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
Text Preprocessing Module for Disaster Tweet Classification

ì´ ëª¨ë“ˆì€ íŠ¸ìœ— í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
This module provides functions to clean tweet text and transform it into a format suitable for machine learning models.
"""

import re
import string
import nltk
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
import emoji
from typing import List, Union
import warnings
warnings.filterwarnings('ignore')

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ / Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

try:
    nltk.data.find('corpora/omw-1.4')
except LookupError:
    nltk.download('omw-1.4')

class TextPreprocessor:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    Class for text preprocessing
    """
    
    def __init__(self, 
                 remove_stopwords: bool = True,
                 lemmatize: bool = True,
                 stem: bool = False,
                 remove_punctuation: bool = True,
                 remove_numbers: bool = False,
                 lowercase: bool = True):
        """
        ì „ì²˜ë¦¬ ì˜µì…˜ ì´ˆê¸°í™” / Initialize preprocessing options
        
        Args:
            remove_stopwords: ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€ / Whether to remove stopwords
            lemmatize: í‘œì œì–´ ì¶”ì¶œ ì—¬ë¶€ / Whether to lemmatize words
            stem: ì–´ê°„ ì¶”ì¶œ ì—¬ë¶€ / Whether to stem words
            remove_punctuation: êµ¬ë‘ì  ì œê±° ì—¬ë¶€ / Whether to remove punctuation
            remove_numbers: ìˆ«ì ì œê±° ì—¬ë¶€ / Whether to remove numbers
            lowercase: ì†Œë¬¸ì ë³€í™˜ ì—¬ë¶€ / Whether to convert to lowercase
        """
        self.remove_stopwords = remove_stopwords
        self.lemmatize = lemmatize
        self.stem = stem
        self.remove_punctuation = remove_punctuation
        self.remove_numbers = remove_numbers
        self.lowercase = lowercase
        
        # NLTK ë„êµ¬ ì´ˆê¸°í™” / Initialize NLTK tools
        if self.remove_stopwords:
            self.stop_words = set(stopwords.words('english'))
        if self.lemmatize:
            self.lemmatizer = WordNetLemmatizer()
        if self.stem:
            self.stemmer = PorterStemmer()
    
    def clean_url(self, text: str) -> str:
        """
        URL ë§í¬ ì œê±° / Remove URL links
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ / Input text
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸ / Cleaned text
        """
        # HTTP/HTTPS URL ì œê±° / Remove HTTP/HTTPS URLs
        text = re.sub(r'http\S+|https\S+|www\.\S+', '', text)
        return text
    
    def clean_mentions_hashtags(self, text: str) -> str:
        """
        ë©˜ì…˜(@)ê³¼ í•´ì‹œíƒœê·¸(#) ì •ì œ / Clean mentions and hashtags
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ / Input text
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸ / Cleaned text
        """
        # @ ë©˜ì…˜ ì œê±° / Remove @ mentions
        text = re.sub(r'@\w+', '', text)
        # # from í•´ì‹œíƒœê·¸ ì œê±°í•˜ë˜ ë‹¨ì–´ëŠ” ìœ ì§€ / Remove # from hashtags but keep words
        text = re.sub(r'#(\w+)', r'\1', text)
        return text
    
    def clean_special_chars(self, text: str) -> str:
        """
        íŠ¹ìˆ˜ ë¬¸ì ì •ì œ / Clean special characters
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ / Input text
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸ / Cleaned text
        """
        # HTML íƒœê·¸ ì œê±° / Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ / Replace consecutive spaces with single space
        text = re.sub(r'\s+', ' ', text)
        
        # ì´ëª¨ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ / Convert emojis to text
        text = emoji.demojize(text, delimiters=(" ", " "))
        
        return text.strip()
    
    def remove_noise(self, text: str) -> str:
        """
        ë…¸ì´ì¦ˆ ì œê±° (ë°˜ë³µ ë¬¸ì, íŠ¹ìˆ˜ ê¸°í˜¸ ë“±) / Remove noise (repeated chars, special symbols)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ / Input text
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸ / Cleaned text
        """
        # ì—°ì†ëœ ê°™ì€ ë¬¸ì 3ê°œ ì´ìƒì„ 2ê°œë¡œ ì¤„ì„ / Reduce 3+ consecutive chars to 2
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
        
        # íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±° (êµ¬ë‘ì  ì œì™¸) / Remove special symbols (except punctuation)
        text = re.sub(r'[^\w\s\.\!\?\,\;\:\'\"]', '', text)
        
        return text
    
    def preprocess_text(self, text: str) -> str:
        """
        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ / Complete preprocessing pipeline
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ / Input text
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ / Preprocessed text
        """
        if pd.isna(text) or text == '':
            return ''
        
        # ê¸°ë³¸ ì •ì œ / Basic cleaning
        text = self.clean_url(text)
        text = self.clean_mentions_hashtags(text)
        text = self.clean_special_chars(text)
        text = self.remove_noise(text)
        
        # ì†Œë¬¸ì ë³€í™˜ / Convert to lowercase
        if self.lowercase:
            text = text.lower()
        
        # í† í°í™” / Tokenization
        tokens = word_tokenize(text)
        
        # ìˆ«ì ì œê±° / Remove numbers
        if self.remove_numbers:
            tokens = [token for token in tokens if not token.isdigit()]
        
        # êµ¬ë‘ì  ì œê±° / Remove punctuation
        if self.remove_punctuation:
            tokens = [token for token in tokens if token not in string.punctuation]
        
        # ë¶ˆìš©ì–´ ì œê±° / Remove stopwords
        if self.remove_stopwords:
            tokens = [token for token in tokens if token.lower() not in self.stop_words]
        
        # í‘œì œì–´ ì¶”ì¶œ / Lemmatization
        if self.lemmatize:
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        
        # ì–´ê°„ ì¶”ì¶œ / Stemming
        if self.stem:
            tokens = [self.stemmer.stem(token) for token in tokens]
        
        # ë¹ˆ í† í° ì œê±° / Remove empty tokens
        tokens = [token for token in tokens if len(token) > 1]
        
        return ' '.join(tokens)
    
    def preprocess_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """
        ë°ì´í„°í”„ë ˆì„ì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬ / Preprocess text column in dataframe
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ / Input dataframe
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… / Text column name
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ / Preprocessed dataframe
        """
        df_copy = df.copy()
        df_copy[f'{text_column}_clean'] = df_copy[text_column].apply(self.preprocess_text)
        return df_copy

def extract_text_features(df: pd.DataFrame, text_column: str) -> pd.DataFrame:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ íŠ¹ì„± ì¶”ì¶œ / Extract additional features from text
    
    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ / Input dataframe
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… / Text column name
        
    Returns:
        íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ / Dataframe with additional features
    """
    df_copy = df.copy()
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ / Text length
    df_copy['text_length'] = df_copy[text_column].str.len()
    
    # ë‹¨ì–´ ìˆ˜ / Word count
    df_copy['word_count'] = df_copy[text_column].str.split().str.len()
    
    # ë¬¸ì¥ ìˆ˜ / Sentence count
    df_copy['sentence_count'] = df_copy[text_column].str.count(r'[.!?]+') + 1
    
    # ëŒ€ë¬¸ì ë¹„ìœ¨ / Uppercase ratio
    df_copy['uppercase_ratio'] = df_copy[text_column].apply(
        lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0
    )
    
    # êµ¬ë‘ì  ìˆ˜ / Punctuation count
    df_copy['punctuation_count'] = df_copy[text_column].str.count(r'[^\w\s]')
    
    # í•´ì‹œíƒœê·¸ ìˆ˜ / Hashtag count
    df_copy['hashtag_count'] = df_copy[text_column].str.count(r'#\w+')
    
    # ë©˜ì…˜ ìˆ˜ / Mention count
    df_copy['mention_count'] = df_copy[text_column].str.count(r'@\w+')
    
    # URL ìˆ˜ / URL count
    df_copy['url_count'] = df_copy[text_column].str.count(r'http\S+|https\S+|www\.\S+')
    
    # í‰ê·  ë‹¨ì–´ ê¸¸ì´ / Average word length
    df_copy['avg_word_length'] = df_copy[text_column].apply(
        lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0
    )
    
    return df_copy

def create_tfidf_features(train_texts: List[str], 
                         test_texts: List[str] = None,
                         max_features: int = 10000,
                         ngram_range: tuple = (1, 2),
                         min_df: int = 2,
                         max_df: float = 0.95) -> tuple:
    """
    TF-IDF ë²¡í„°í™” ìˆ˜í–‰ / Perform TF-IDF vectorization
    
    Args:
        train_texts: í›ˆë ¨ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ / Training text list
        test_texts: í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ / Test text list
        max_features: ìµœëŒ€ íŠ¹ì„± ìˆ˜ / Maximum number of features
        ngram_range: N-gram ë²”ìœ„ / N-gram range
        min_df: ìµœì†Œ ë¬¸ì„œ ë¹ˆë„ / Minimum document frequency
        max_df: ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ / Maximum document frequency
        
    Returns:
        (í›ˆë ¨ íŠ¹ì„±, í…ŒìŠ¤íŠ¸ íŠ¹ì„±, ë²¡í„°ë¼ì´ì €) / (train features, test features, vectorizer)
    """
    # TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™” / Initialize TF-IDF vectorizer
    vectorizer = TfidfVectorizer(
        max_features=max_features,
        ngram_range=ngram_range,
        min_df=min_df,
        max_df=max_df,
        stop_words='english'
    )
    
    # í›ˆë ¨ ë°ì´í„°ë¡œ ë²¡í„°ë¼ì´ì € í•™ìŠµ ë° ë³€í™˜ / Fit and transform training data
    train_features = vectorizer.fit_transform(train_texts)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ (ìˆëŠ” ê²½ìš°) / Transform test data (if provided)
    test_features = None
    if test_texts is not None:
        test_features = vectorizer.transform(test_texts)
    
    return train_features, test_features, vectorizer

def preprocess_disaster_tweets(df: pd.DataFrame, 
                              text_column: str = 'text',
                              is_training: bool = True) -> pd.DataFrame:
    """
    ì¬í•´ íŠ¸ìœ— ë°ì´í„°ì…‹ ì „ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜ / Preprocessing function specific for disaster tweet dataset
    
    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ / Input dataframe
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… / Text column name
        is_training: í›ˆë ¨ ë°ì´í„° ì—¬ë¶€ / Whether it's training data
        
    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ / Preprocessed dataframe
    """
    # ì „ì²˜ë¦¬ ê°ì²´ ìƒì„± / Create preprocessor object
    preprocessor = TextPreprocessor(
        remove_stopwords=True,
        lemmatize=True,
        stem=False,
        remove_punctuation=True,
        remove_numbers=False,
        lowercase=True
    )
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ / Preprocess text
    df_processed = preprocessor.preprocess_dataframe(df, text_column)
    
    # ì¶”ê°€ íŠ¹ì„± ì¶”ì¶œ / Extract additional features
    df_processed = extract_text_features(df_processed, text_column)
    
    # í‚¤ì›Œë“œì™€ ìœ„ì¹˜ ì •ë³´ ì²˜ë¦¬ / Process keyword and location info
    if 'keyword' in df_processed.columns:
        df_processed['has_keyword'] = df_processed['keyword'].notna().astype(int)
        df_processed['keyword_clean'] = df_processed['keyword'].fillna('').str.lower()
    
    if 'location' in df_processed.columns:
        df_processed['has_location'] = df_processed['location'].notna().astype(int)
        df_processed['location_clean'] = df_processed['location'].fillna('').str.lower()
    
    return df_processed

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ / Test code
    sample_tweets = [
        "URGENT: Forest fire near residential area! Evacuate immediately! #emergency #fire",
        "I'm literally on fire today! Crushed my presentation ğŸ”¥ #success #motivated",
        "Breaking: Earthquake hits the city center. Magnitude 6.2 recorded.",
        "This traffic jam is killing me... stuck for 2 hours already"
    ]
    
    # ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ / Test preprocessing
    preprocessor = TextPreprocessor()
    
    print("ì›ë³¸ íŠ¸ìœ—ë“¤ / Original tweets:")
    for i, tweet in enumerate(sample_tweets, 1):
        print(f"{i}. {tweet}")
    
    print("\nì „ì²˜ë¦¬ëœ íŠ¸ìœ—ë“¤ / Preprocessed tweets:")
    for i, tweet in enumerate(sample_tweets, 1):
        cleaned = preprocessor.preprocess_text(tweet)
        print(f"{i}. {cleaned}") 