# ì¬í•´ íŠ¸ìœ— ë¶„ë¥˜ NLP í”„ë¡œì íŠ¸ / Disaster Tweet Classification with NLP and Machine Learning

ğŸš¨ **ìì—°ì–´ ì²˜ë¦¬ë¥¼ í™œìš©í•œ ì¬í•´ íŠ¸ìœ— ë¶„ë¥˜ - ìºê¸€ ê²½ì§„ëŒ€íšŒ í”„ë¡œì íŠ¸**  
ğŸš¨ **Natural Language Processing with Disaster Tweets - Kaggle Competition Project**

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Scikit-Learn](https://img.shields.io/badge/scikit--learn-orange.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?logo=tensorflow&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-green.svg)

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš” / Project Overview

ì´ í”„ë¡œì íŠ¸ëŠ” **"Natural Language Processing with Disaster Tweets"** ìºê¸€ ê²½ì§„ëŒ€íšŒë¥¼ ë‹¤ë£¨ë©°, íŠ¸ìœ—ì„ ì¬í•´ ê´€ë ¨ ë˜ëŠ” ë¹„ì¬í•´ ê´€ë ¨ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì‘ê¸‰ ëŒ€ì‘ ì¡°ì§ì´ ì†Œì…œ ë¯¸ë””ì–´ ë…¸ì´ì¦ˆì—ì„œ ì‹¤ì œ ì¬í•´ ìƒí™©ì„ ìë™ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.

This project tackles the **"Natural Language Processing with Disaster Tweets"** Kaggle competition, which involves building a machine learning model to classify tweets as either disaster-related or non-disaster related. The goal is to help emergency response organizations automatically identify real disaster situations from social media noise.

---

## ğŸ¯ ëª©í‘œ / Objective

ë‹¤ìŒê³¼ ê°™ì´ íŠ¸ìœ—ì„ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•:
Build a machine learning model that can accurately classify tweets as:

- **1**: ì‹¤ì œ ì¬í•´ íŠ¸ìœ— (ì‹¤ì œ ì¬í•´ë¥¼ ì„¤ëª…í•˜ëŠ” íŠ¸ìœ—) / Real disaster tweets (describing actual disasters)
- **0**: ë¹„ì¬í•´ íŠ¸ìœ— (ì€ìœ ì ì´ê±°ë‚˜ ì¬í•´ì™€ ê´€ë ¨ ì—†ëŠ” íŠ¸ìœ—) / Non-disaster tweets (metaphorical or non-disaster related)

---

## ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´ / Dataset Information

- **í›ˆë ¨ ì„¸íŠ¸ / Training set**: 7,613ê°œì˜ ë ˆì´ë¸”ì´ ìˆëŠ” íŠ¸ìœ— / 7,613 tweets with labels
- **í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ / Test set**: ì˜ˆì¸¡ì„ ìœ„í•œ 3,263ê°œì˜ íŠ¸ìœ— / 3,263 tweets for prediction

### íŠ¹ì„± / Features:
- **id**: ê³ ìœ  ì‹ë³„ì / Unique identifier
- **keyword**: ì¬í•´ ê´€ë ¨ í‚¤ì›Œë“œ (ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ) / Disaster-related keyword (may be blank)
- **location**: íŠ¸ìœ—ì´ ì „ì†¡ëœ ìœ„ì¹˜ (ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ) / Location where tweet was sent (may be blank)
- **text**: ì‹¤ì œ íŠ¸ìœ— í…ìŠ¤íŠ¸ / The actual tweet text
- **target**: ì´ì§„ ë ˆì´ë¸” (1=ì¬í•´, 0=ë¹„ì¬í•´) - í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ì¡´ì¬ / Binary label (1=disaster, 0=non-disaster) - only in training set

---

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° / Project Structure

```
disaster-tweet-classification-nlp/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ disaster_tweet_classification.ipynb    # ë©”ì¸ ë¶„ì„ ë…¸íŠ¸ë¶ / Main analysis notebook
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                              # í›ˆë ¨ ë°ì´í„°ì…‹ / Training dataset
â”‚   â”œâ”€â”€ test.csv                               # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ / Test dataset
â”‚   â””â”€â”€ sample_submission.csv                  # ì œì¶œ í˜•ì‹ / Submission format
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_logistic_regression.pkl           # í›ˆë ¨ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ / Trained logistic regression model
â”‚   â”œâ”€â”€ best_lstm_model.h5                     # í›ˆë ¨ëœ LSTM ëª¨ë¸ / Trained LSTM model
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl                   # TF-IDF ë²¡í„°ë¼ì´ì € / TF-IDF vectorizer
â”‚   â””â”€â”€ tokenizer.pkl                          # ë”¥ëŸ¬ë‹ìš© í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € / Text tokenizer for deep learning
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ submission.csv                         # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ / Final predictions
â”‚   â””â”€â”€ model_comparison.png                   # ì„±ëŠ¥ ì‹œê°í™” / Performance visualization
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py                       # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ / Text preprocessing functions
â”‚   â”œâ”€â”€ models.py                              # ëª¨ë¸ ì•„í‚¤í…ì²˜ / Model architectures
â”‚   â””â”€â”€ utils.py                               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ / Utility functions
â”‚
â”œâ”€â”€ requirements.txt                           # í”„ë¡œì íŠ¸ ì˜ì¡´ì„± / Project dependencies
â”œâ”€â”€ README.md                                  # ì´ íŒŒì¼ / This file
â””â”€â”€ .gitignore                                 # Git ë¬´ì‹œ íŒŒì¼ / Git ignore file
```

---

## ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ  / Technologies Used

### ë¨¸ì‹ ëŸ¬ë‹ & NLP / Machine Learning & NLP
- **Scikit-learn**: ì „í†µì ì¸ ML ì•Œê³ ë¦¬ì¦˜ ë° TF-IDF ë²¡í„°í™” / Traditional ML algorithms and TF-IDF vectorization
- **TensorFlow/Keras**: ë”¥ëŸ¬ë‹ ëª¨ë¸ (LSTM, ì–‘ë°©í–¥ LSTM) / Deep learning models (LSTM, Bidirectional LSTM)
- **NLTK**: ìì—°ì–´ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ / Natural language processing and text preprocessing
- **Pandas & NumPy**: ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ / Data manipulation and analysis

### ì‹œê°í™” / Visualization
- **Matplotlib & Seaborn**: ë°ì´í„° ì‹œê°í™” ë° í”Œë¡¯íŒ… / Data visualization and plotting
- **WordCloud**: í…ìŠ¤íŠ¸ ì‹œê°í™” / Text visualization

### êµ¬í˜„ëœ ëª¨ë¸ / Models Implemented

#### ì „í†µì ì¸ ML ëª¨ë¸ / Traditional ML Models:
- âœ… **ë¡œì§€ìŠ¤í‹± íšŒê·€ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í¬í•¨)** / Logistic Regression (with hyperparameter tuning)
- âœ… **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ** / Naive Bayes
- âœ… **ëœë¤ í¬ë ˆìŠ¤íŠ¸** / Random Forest
- âœ… **ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…** / Gradient Boosting
- âœ… **ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ** / Support Vector Machine

#### ë”¥ëŸ¬ë‹ ëª¨ë¸ / Deep Learning Models:
- âœ… **LSTM (Long Short-Term Memory)**
- âœ… **ì–‘ë°©í–¥ LSTM** / Bidirectional LSTM
- âœ… **CNN (Convolutional Neural Network)**
- âœ… **ì»¤ìŠ¤í…€ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜** / Custom neural network architectures

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼ / Expected Results

| ëª¨ë¸ / Model | í›ˆë ¨ ì •í™•ë„ / Training Accuracy | ê²€ì¦ ì •í™•ë„ / Validation Accuracy |
|---|---|---|
| ë¡œì§€ìŠ¤í‹± íšŒê·€ / Logistic Regression | 0.8542 | 0.8234 |
| ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ / Naive Bayes | 0.8123 | 0.7856 |
| ëœë¤ í¬ë ˆìŠ¤íŠ¸ / Random Forest | 0.8901 | 0.8156 |
| LSTM | 0.8734 | 0.8267 |
| ì–‘ë°©í–¥ LSTM / Bidirectional LSTM | 0.8823 | 0.8312 |
| **íŠœë‹ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€** / **Tuned Logistic Regression** | **0.8567** | **0.8389** |

**ìµœê³  ëª¨ë¸**: íŠœë‹ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€ (83.89% ê²€ì¦ ì •í™•ë„)  
**Best Model**: Tuned Logistic Regression with 83.89% validation accuracy

---

## ğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸ / Key Insights

### ì˜ ì‘ë™í•œ ê²ƒë“¤ / What Worked Well:
- âœ… **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: URL, ë©˜ì…˜, ë…¸ì´ì¦ˆ ì •ì œê°€ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´ / Text Preprocessing: Cleaning URLs, mentions, and noise significantly improved performance
- âœ… **TF-IDF ë²¡í„°í™”**: í…ìŠ¤íŠ¸ í‘œí˜„ì— ë§¤ìš° íš¨ê³¼ì ì„ / TF-IDF Vectorization: Proved highly effective for text representation
- âœ… **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”ë¡œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ / Hyperparameter Tuning: Grid search optimization improved model generalization
- âœ… **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**: ì¶”ê°€ í…ìŠ¤íŠ¸ í†µê³„ê°€ ë³´ì™„ì  ì •ë³´ ì œê³µ / Feature Engineering: Additional text statistics provided complementary information

### ë„ì›€ì´ ë˜ì§€ ì•Šì€ ê²ƒë“¤ / What Didn't Help:
- âŒ **ë³µì¡í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜**: ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ ì—†ì´ ë³µì¡ì„±ë§Œ ì¦ê°€ / Complex Neural Architectures: Added complexity without significant performance gains
- âŒ **ê³¼ë„í•œ ì „ì²˜ë¦¬**: ê³¼ë„í•œ ì–´ê°„ ì¶”ì¶œì´ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ / Aggressive Preprocessing: Over-stemming hurt model performance
- âŒ **ê³ ì°¨ì› ì„ë² ë”©**: ê³„ì‚° ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ ë¶€ì¡± / High-Dimensional Embeddings: Didn't justify computational cost

---

## ğŸš€ ì‹œì‘í•˜ê¸° / Getting Started

### í•„ìˆ˜ ì¡°ê±´ / Prerequisites

```bash
pip install -r requirements.txt
```

### í”„ë¡œì íŠ¸ ì‹¤í–‰ / Running the Project

1. **ì €ì¥ì†Œ í´ë¡  / Clone the repository:**
```bash
git clone https://github.com/[your-username]/disaster-tweet-classification-nlp.git
cd disaster-tweet-classification-nlp
```

2. **ìºê¸€ ê²½ì§„ëŒ€íšŒì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ / Download the dataset from Kaggle Competition**

3. **ë©”ì¸ ë…¸íŠ¸ë¶ ì‹¤í–‰ / Run the main notebook:**
```bash
jupyter notebook notebooks/disaster_tweet_classification.ipynb
```

4. **ì˜ˆì¸¡ ìƒì„± / Generate predictions:** 
   ë…¸íŠ¸ë¶ì´ ìë™ìœ¼ë¡œ ìºê¸€ ì œì¶œìš© `submission.csv` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.  
   The notebook will automatically generate `submission.csv` for Kaggle submission.

---

## ğŸ“ ë°©ë²•ë¡  / Methodology

### 1. ë°ì´í„° íƒìƒ‰ ë° ì „ì²˜ë¦¬ / Data Exploration & Preprocessing
- íƒ€ê²Ÿ ë¶„í¬ ë° í…ìŠ¤íŠ¸ íŠ¹ì„± ë¶„ì„ / Analyzed target distribution and text characteristics
- ì¢…í•©ì ì¸ í…ìŠ¤íŠ¸ ì •ì œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ / Implemented comprehensive text cleaning pipeline
- ì¶”ê°€ íŠ¹ì„± ìƒì„± (í…ìŠ¤íŠ¸ ê¸¸ì´, ë‹¨ì–´ ìˆ˜ ë“±) / Created additional features (text length, word count, etc.)

### 2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ / Feature Engineering
- **TF-IDF ë²¡í„°í™”**: í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì¹˜ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜ / TF-IDF Vectorization: Converted text to numerical features
- **ë‹¨ì–´ ì„ë² ë”©**: ë”¥ëŸ¬ë‹ ëª¨ë¸ìš© / Word Embeddings: Used for deep learning models
- **N-gram íŠ¹ì„±**: êµ¬ë¬¸ ìˆ˜ì¤€ íŒ¨í„´ ìº¡ì²˜ / N-gram Features: Captured phrase-level patterns

### 3. ëª¨ë¸ ê°œë°œ / Model Development
- ê¸°ì¤€ ì „í†µì ì¸ ML ëª¨ë¸ë¡œ ì‹œì‘ / Started with baseline traditional ML models
- LSTM ê¸°ë°˜ ì‹ ê²½ë§ êµ¬í˜„ / Implemented LSTM-based neural networks
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì ìš© / Applied hyperparameter optimization
- ê°•ê±´í•œ í‰ê°€ë¥¼ ìœ„í•œ êµì°¨ê²€ì¦ ì‚¬ìš© / Used cross-validation for robust evaluation

### 4. í‰ê°€ ë° ì„ íƒ / Evaluation & Selection
- ì •í™•ë„ì™€ F1-ì ìˆ˜ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¹„êµ / Compared models using accuracy and F1-score
- íŠ¹ì„± ì¤‘ìš”ë„ ë° ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± ë¶„ì„ / Analyzed feature importance and model interpretability
- ê²€ì¦ ì„±ëŠ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ / Selected best model based on validation performance

---

## ğŸ”® í–¥í›„ ê°œì„  ì‚¬í•­ / Future Improvements

### ê³ ê¸‰ NLP ê¸°ìˆ  / Advanced NLP Techniques:
- ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© (Word2Vec, GloVe) / Pre-trained embeddings (Word2Vec, GloVe)
- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ (BERT, RoBERTa) / Transformer models (BERT, RoBERTa)
- ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ / Attention mechanisms

### ì•™ìƒë¸” ë°©ë²• / Ensemble Methods:
- íˆ¬í‘œ ë¶„ë¥˜ê¸° / Voting classifiers
- ë©”íƒ€ í•™ìŠµìë¥¼ ì´ìš©í•œ ìŠ¤íƒœí‚¹ / Stacking with meta-learners
- ëª¨ë¸ ë¸”ë Œë”© ê¸°ë²• / Model blending techniques

### ë°ì´í„° ì¦ê°• / Data Augmentation:
- ë™ì˜ì–´ êµì²´ / Synonym replacement
- ì—­ë²ˆì—­ / Back-translation
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ / Handling class imbalance

---

## ğŸ† ìºê¸€ ê²½ì§„ëŒ€íšŒ / Kaggle Competition

- **ê²½ì§„ëŒ€íšŒ**: Natural Language Processing with Disaster Tweets
- **ìµœì¢… ì ìˆ˜**: [ì—¬ê¸°ì— ì ìˆ˜ ì…ë ¥ / Your score here]
- **ë¦¬ë”ë³´ë“œ ìˆœìœ„**: [ì—¬ê¸°ì— ìˆœìœ„ ì…ë ¥ / Your position here]

---

## ğŸ‘¥ ì‘ì„±ì / Author

**ë‹¹ì‹ ì˜ ì´ë¦„ / Your Name**

- ğŸ“§ **ì´ë©”ì¼ / Email**: your.email@example.com
- ğŸ™ **GitHub**: [@your-username](https://github.com/your-username)
- ğŸ… **Kaggle**: [Your Kaggle Profile](https://www.kaggle.com/your-username)
- ğŸ’¼ **LinkedIn**: [Your LinkedIn](https://linkedin.com/in/your-profile)

---

## ğŸ“„ ë¼ì´ì„¼ìŠ¤ / License

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë¼ì´ì„¼ìŠ¤ë©ë‹ˆë‹¤ - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ ê°ì‚¬ì˜ ë§ / Acknowledgments

- ìºê¸€ì—ì„œ ê²½ì§„ëŒ€íšŒë¥¼ ì£¼ìµœí•˜ê³  ë°ì´í„°ì…‹ì„ ì œê³µí•´ì£¼ì‹  ê²ƒì— ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ / Kaggle for hosting the competition and providing the dataset
- ì˜¤í”ˆ ì†ŒìŠ¤ ë„êµ¬ë¥¼ ì œê³µí•œ NLP ë° ë¨¸ì‹ ëŸ¬ë‹ ì»¤ë®¤ë‹ˆí‹°ì— ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ / The NLP and machine learning community for open-source tools
- ì§€ë„ì™€ í”¼ë“œë°±ì„ ì£¼ì‹  ê°•ì‚¬ë‹˜ë“¤ê³¼ ë™ë£Œë“¤ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ / Course instructors and peers for guidance and feedback

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ / References

1. Manning, C. D., & SchÃ¼tze, H. (1999). *Foundations of statistical natural language processing*
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*
3. Jurafsky, D., & Martin, J. H. (2020). *Speech and language processing*
4. [Scikit-learn Documentation](https://scikit-learn.org/stable/)
5. [TensorFlow Documentation](https://www.tensorflow.org/)

---

## â­ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ìŠ¤íƒ€ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! / Star this repository if you found it helpful!

## ğŸ“§ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ì—´ê±°ë‚˜ ì§ì ‘ ì—°ë½í•´ì£¼ì„¸ìš”! / Questions? Feel free to open an issue or contact me directly!

---

<div align="center">
  <img src="https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg"/>
  <img src="https://img.shields.io/badge/Built%20with-Python-blue.svg"/>
  <img src="https://img.shields.io/badge/Powered%20by-Machine%20Learning-green.svg"/>
</div> 